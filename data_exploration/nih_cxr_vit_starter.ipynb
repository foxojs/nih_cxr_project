{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "import torch \n",
    "import matplotlib.pyplot as plt \n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "from tqdm import trange, tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "import os \n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#If you haven't downloaded already, this takes aroun  40GB and around half an hour to download all the data (note that we split here)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# it will be cached on your system, so you access it in the future by running this code (rather than opening the files per se)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# once you've downloaded, hugging face automatically checks to see if you've already downloaded so subsequent loads are quick \u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# we use just a small portion here to get our code working \u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# hugging face has already split in to train and test so we can use train here and make a validation subset in our custom dataset class \u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m ds_train \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malkzar90/NIH-Chest-X-ray-dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage-classification\u001b[39m\u001b[38;5;124m'\u001b[39m, split \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain[:500]\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# we hold back our test data to be used purely for testing, not in the context of our training loop \u001b[39;00m\n\u001b[1;32m     10\u001b[0m ds_test \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malkzar90/NIH-Chest-X-ray-dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage-classification\u001b[39m\u001b[38;5;124m'\u001b[39m, split \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest[:500]\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#If you haven't downloaded already, this takes aroun  40GB and around half an hour to download all the data (note that we split here)\n",
    "# it will be cached on your system, so you access it in the future by running this code (rather than opening the files per se)\n",
    "# once you've downloaded, hugging face automatically checks to see if you've already downloaded so subsequent loads are quick \n",
    "\n",
    "# we use just a small portion here to get our code working \n",
    "# hugging face has already split in to train and test so we can use train here and make a validation subset in our custom dataset class \n",
    "ds_train = load_dataset(\"alkzar90/NIH-Chest-X-ray-dataset\", 'image-classification', split = \"train[:500]\") \n",
    "\n",
    "# we hold back our test data to be used purely for testing, not in the context of our training loop \n",
    "ds_test = load_dataset(\"alkzar90/NIH-Chest-X-ray-dataset\", 'image-classification', split = \"test[:500]\") \n",
    "\n",
    "# you can view a single image to check things have worked \n",
    "ds_train[300]['image']\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 46863.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(4)):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patches(image_tensor, patch_size = 4):\n",
    "    bs, c, h, w = image_tensor.size()\n",
    "\n",
    "    # define teh unfold layer with appropriate parameters \n",
    "\n",
    "    unfold = torch.nn.Unfold(kernel_size = patch_size, stride = patch_size)\n",
    "\n",
    "    unfolded = unfold(image_tensor)\n",
    "\n",
    "    # reshape the unfolded tensor to match the desired output shape \n",
    "    # output shaep BS x L x C x 8 x8 where L is the number of patches in each dimension \n",
    "    # fo reach dimension, number of patches = (original dimension size) //patch_size \n",
    "\n",
    "    unfolded = unfolded.transpose(1, 2).reshape(bs, -1, c * patch_size * patch_size)\n",
    "\n",
    "    return unfolded\n",
    "\n",
    "\n",
    "# we have a hugging face dataset, so we now define a custom dataset class which processes this, ensures our labels are one hot encoded etc. \n",
    "\n",
    "class MultiLabelDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Initialize with a Hugging Face dataset that's already formatted as torch tensors. \n",
    "    Will convert to tensors if plain hugging face dataset. \n",
    "    Will handle the multi label nature of our data through one hot encoding \n",
    "    \n",
    "    Args:\n",
    "        hf_dataset: A Hugging Face dataset with 'image' and 'labels' columns\n",
    "    \"\"\"\n",
    "      \n",
    "    def __init__(self, hf_dataset, image_size):\n",
    "\n",
    "        self.x_train, self.x_val, self.y_train, self.y_val = None, None, None, None\n",
    "        self.mode = \"train\"\n",
    "\n",
    "        hf_dataset = hf_dataset.with_format(\"torch\")\n",
    "        print(hf_dataset.format)\n",
    "\n",
    "        self.processed_images = []\n",
    "        self.processed_labels = []\n",
    "\n",
    "        for sample in tqdm(hf_dataset, desc = \"processing images\", leave= True):\n",
    "            image = sample['image']\n",
    "\n",
    "            # we resize our image if specified \n",
    "            \n",
    "            image = F.interpolate(image.unsqueeze(0), size = image_size, mode = \"bilinear\").squeeze(0)\n",
    "\n",
    "\n",
    "            if image.shape[0] == 4:\n",
    "                image = torch.index_select(image, 0, torch.tensor([0]))\n",
    "\n",
    "            # normalize pixel values \n",
    "            image = image/255\n",
    "\n",
    "            if image.shape[0] == 1:\n",
    "                image = image.repeat(3, 1, 1) # for convolutional networks we need 3 channels, remove this line and line below if wanting 1024, 1024 shape \n",
    "\n",
    "            # image = image.permute(1, 2, 0) # channel dimension needs to be the last one, not first \n",
    "\n",
    "            labels = sample['labels']\n",
    "            one_hot = torch.zeros(15, dtype = torch.long)\n",
    "            one_hot[labels] = 1\n",
    "            self.processed_images.append(image)\n",
    "            self.processed_labels.append(one_hot)\n",
    "    \n",
    "    def train_validation_split(self):\n",
    "        \"\"\"\n",
    "        Takes our training data and produces a validation set from our training data\n",
    "        Ensures that we don't use our hugging face defined test set during the training process \n",
    "        Means that we will assess trained models on a totally separate test set to avoid overfitting on test set\n",
    "        Note that we use a subset of train as a validation set \n",
    "        \"\"\"\n",
    "        self.x_train, self.x_val, self.y_train, self.y_val = train_test_split(self.processed_images, self.processed_labels, test_size = 0.2,\n",
    "                                                                              random_state = 42)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Returns the length of our training or validation set depending on mode \"\"\"\n",
    "        if self.mode == \"train\":\n",
    "            return len(self.x_train)\n",
    "        elif self.mode == \"val\":\n",
    "            return len(self.x_val)\n",
    "        elif self.mode == \"test\":\n",
    "            return len(self.processed_images)\n",
    "    \n",
    "        \n",
    "\n",
    "    # note we are not doing lazy processing, so our data is processed when the dataset is instantiated. \n",
    "    # here we will return a train test split of our hugging face training data, unless we're using the test data in which case we return it all \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Gets items from either the training or validation set depending on mode\"\"\"\n",
    "        if self.mode == \"train\":\n",
    "            return {\"image\": self.x_train[idx], \"labels\": self.y_train[idx]}\n",
    "        elif self.mode == \"val\":\n",
    "            return {\"image\": self.x_val[idx], \"labels\": self.y_val[idx]}\n",
    "        elif self.mode == \"test\":\n",
    "            return {\"image\": self.processed_images[idx], \"labels\": self.processed_labels[idx]}\n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "# we create patches \n",
    "# we embed these patches by using encoder only transformer \n",
    "# then we pass them through an encoder only transformer \n",
    "# use a transformer - self attention mixes spatial regions of an image earlier on (rather than convolutions which takes time to get entire spatial field of image)\n",
    "# by treating every pixel as an embedding in a sequence, each spatial region can query all other spatial regions in image - gives context \n",
    "\n",
    "# neeed to start by writing a transformer block class with self attention \n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, hidden_size = 128, num_heads = 4):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "\n",
    "\n",
    "        # layer normalisation to normalize the input data \n",
    "        self.norm1 = nn.LayerNorm(hidden_size)\n",
    "\n",
    "\n",
    "        # multi head attention mechanism \n",
    "\n",
    "        self.multihead_attn = nn.MultiheadAttention(hidden_size, num_heads=num_heads, \n",
    "                                                    batch_first = True, dropout = 0.1)\n",
    "        \n",
    "        # another layer of normalisation \n",
    "\n",
    "        self.norm2 = nn.LayerNorm(hidden_size)\n",
    "\n",
    "        # multi layer perceptron with a hidden layer and activation function \n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size *2), \n",
    "            nn.LayerNorm(hidden_size * 2),\n",
    "            nn.ELU(), \n",
    "            nn.Linear(hidden_size * 2, hidden_size))\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # apply the first layer of normalisation \n",
    "\n",
    "        norm_x = self.norm1(x)\n",
    "\n",
    "        # apply multi headed attention and add the input (residual connection)\n",
    "\n",
    "        x = self.multihead_attn(norm_x, norm_x, norm_x)[0] + x\n",
    "\n",
    "        # apply second layer of normalisation \n",
    "\n",
    "        norm_x = self.norm2(x)\n",
    "\n",
    "        # pass through the mlp and add the input (Residual connection)\n",
    "\n",
    "        x = self.mlp(norm_x) + x\n",
    "\n",
    "        return x \n",
    "        \n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, image_size, channels_in, patch_size, hidden_size,\n",
    "                 num_layers, num_heads = 8):\n",
    "        super(ViT, self).__init__()\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        # fully connected layer to project input patches to the hidden size dimension \n",
    "\n",
    "        self.fc_in = nn.Linear(channels_in * patch_size * patch_size, hidden_size) # this is causing a problem atm \n",
    "\n",
    "        # create list of transformer blocks \n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(hidden_size, num_heads) for _ in range(num_layers)])\n",
    "        \n",
    "        # fully connected output layer to map to the number of classes \n",
    "\n",
    "        self.fc_out = nn.Linear(hidden_size, 15)\n",
    "\n",
    "        # parameter for the output token \n",
    "\n",
    "        self.out_vec = nn.Parameter(torch.zeros(1, 1, hidden_size))\n",
    "\n",
    "        # positional embeddings to retain positional information of patches \n",
    "\n",
    "        seq_length = (image_size //patch_size) **2\n",
    "        self.pos_embedding = nn.Parameter(torch.empty(1, seq_length, hidden_size).normal_(std = 0.001))\n",
    "\n",
    "    def forward(self, image):\n",
    "\n",
    "        bs = image.shape[0]\n",
    "\n",
    "        # extract patches from the image and flatten them \n",
    "\n",
    "        patch_seq = extract_patches(image, patch_size = self.patch_size)\n",
    "\n",
    "        \n",
    "\n",
    "        # project patches to the hidden size dimension \n",
    "\n",
    "        patch_emb = self.fc_in(patch_seq)\n",
    "\n",
    "\n",
    "        # add positional embeddings to the patch embeddings \n",
    "\n",
    "        patch_emb = patch_emb + self.pos_embedding\n",
    "\n",
    "        # concatenate the output token to the patch embeddings \n",
    "\n",
    "        embs = torch.cat((self.out_vec.expand(bs, 1, -1), patch_emb), 1)\n",
    "\n",
    "        # pass embeddings through each transformer block \n",
    "\n",
    "        for block in self.blocks: \n",
    "            embs = block(embs)\n",
    "\n",
    "        return self.fc_out(embs[:, 0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'torch', 'format_kwargs': {}, 'columns': ['image', 'labels'], 'output_all_columns': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing images: 100%|██████████| 500/500 [00:04<00:00, 122.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the size of training data is: 400\n",
      " the size of validation data is: 100\n",
      "{'type': 'torch', 'format_kwargs': {}, 'columns': ['image', 'labels'], 'output_all_columns': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing images: 100%|██████████| 500/500 [00:03<00:00, 129.68it/s]\n"
     ]
    }
   ],
   "source": [
    "# now we can produce our train and validation dataloaders which will be used in training later\n",
    "training_dataset_class = MultiLabelDataset(ds_train, image_size = (128, 128)) # make sure to have the channel dimension\n",
    "training_dataset_class.train_validation_split()\n",
    "\n",
    "# set dataset class mode to train to generate a training split \n",
    "training_dataset_class.mode = \"train\"\n",
    "print(f\"the size of training data is: {len(training_dataset_class)}\")\n",
    "train_dataloader = DataLoader(training_dataset_class, batch_size = 4, shuffle = True)\n",
    "\n",
    "# set dataset class mode to val to generate a validation split \n",
    "\n",
    "training_dataset_class.mode = \"val\"\n",
    "print(f\" the size of validation data is: {len(training_dataset_class)}\")\n",
    "val_dataloader = DataLoader(training_dataset_class, batch_size = 4, shuffle = True)\n",
    "\n",
    "\n",
    "\n",
    "test_dataset_class = MultiLabelDataset(ds_test, image_size = (128, 128))\n",
    "test_dataset_class.mode = \"test\"\n",
    "test_dataloader = DataLoader(test_dataset_class, batch_size = 4, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.MultiLabelDataset at 0x3211c8850>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'torch', 'format_kwargs': {}, 'columns': ['image', 'labels'], 'output_all_columns': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing images: 100%|██████████| 500/500 [00:16<00:00, 29.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the size of training data is: 400\n",
      " the size of validation data is: 100\n",
      "{'type': 'torch', 'format_kwargs': {}, 'columns': ['image', 'labels'], 'output_all_columns': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing images: 100%|██████████| 500/500 [00:16<00:00, 29.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model works with your data\n",
      "Output shape: torch.Size([4, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "training:   0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "training:   4%|▍         | 1/25 [00:04<01:40,  4.18s/it]\u001b[A\n",
      "training:   8%|▊         | 2/25 [00:08<01:35,  4.17s/it]\u001b[A\n",
      "training:  12%|█▏        | 3/25 [00:12<01:31,  4.16s/it]\u001b[A\n",
      "training:  16%|█▌        | 4/25 [00:16<01:27,  4.15s/it]\u001b[A\n",
      "training:  20%|██        | 5/25 [00:20<01:22,  4.14s/it]\u001b[A\n",
      "training:  24%|██▍       | 6/25 [00:24<01:18,  4.14s/it]\u001b[A\n",
      "training:  28%|██▊       | 7/25 [00:29<01:14,  4.14s/it]\u001b[A\n",
      "training:  32%|███▏      | 8/25 [00:33<01:10,  4.13s/it]\u001b[A\n",
      "training:  36%|███▌      | 9/25 [00:37<01:06,  4.13s/it]\u001b[A\n",
      "training:  40%|████      | 10/25 [00:41<01:01,  4.13s/it]\u001b[A\n",
      "training:  44%|████▍     | 11/25 [00:45<00:57,  4.13s/it]\u001b[A\n",
      "training:  48%|████▊     | 12/25 [00:49<00:53,  4.13s/it]\u001b[A\n",
      "training:  52%|█████▏    | 13/25 [00:53<00:49,  4.13s/it]\u001b[A\n",
      "training:  56%|█████▌    | 14/25 [00:57<00:45,  4.13s/it]\u001b[A\n",
      "training:  60%|██████    | 15/25 [01:02<00:41,  4.14s/it]\u001b[A\n",
      "training:  64%|██████▍   | 16/25 [01:06<00:37,  4.14s/it]\u001b[A\n",
      "training:  68%|██████▊   | 17/25 [01:10<00:33,  4.14s/it]\u001b[A\n",
      "training:  72%|███████▏  | 18/25 [01:14<00:28,  4.14s/it]\u001b[A\n",
      "training:  76%|███████▌  | 19/25 [01:18<00:24,  4.14s/it]\u001b[A\n",
      "training:  80%|████████  | 20/25 [01:22<00:20,  4.13s/it]\u001b[A\n",
      "training:  84%|████████▍ | 21/25 [01:26<00:16,  4.13s/it]\u001b[A\n",
      "training:  88%|████████▊ | 22/25 [01:31<00:12,  4.13s/it]\u001b[A\n",
      "training:  92%|█████████▏| 23/25 [01:35<00:08,  4.13s/it]\u001b[A\n",
      "training:  96%|█████████▌| 24/25 [01:39<00:04,  4.13s/it]\u001b[A\n",
      "training: 100%|██████████| 25/25 [01:43<00:00,  4.14s/it]\n",
      "\n",
      "Evaluating:   0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "Evaluating:   4%|▍         | 1/25 [00:00<00:15,  1.50it/s]\u001b[A\n",
      "Evaluating:   8%|▊         | 2/25 [00:01<00:15,  1.50it/s]\u001b[A\n",
      "Evaluating:  12%|█▏        | 3/25 [00:02<00:14,  1.50it/s]\u001b[A\n",
      "Evaluating:  16%|█▌        | 4/25 [00:02<00:14,  1.49it/s]\u001b[A\n",
      "Evaluating:  20%|██        | 5/25 [00:03<00:13,  1.50it/s]\u001b[A\n",
      "Evaluating:  24%|██▍       | 6/25 [00:04<00:12,  1.50it/s]\u001b[A\n",
      "Evaluating:  28%|██▊       | 7/25 [00:04<00:12,  1.49it/s]\u001b[A\n",
      "Evaluating:  32%|███▏      | 8/25 [00:05<00:11,  1.49it/s]\u001b[A\n",
      "Evaluating:  36%|███▌      | 9/25 [00:06<00:10,  1.49it/s]\u001b[A\n",
      "Evaluating:  40%|████      | 10/25 [00:06<00:10,  1.49it/s]\u001b[A\n",
      "Evaluating:  44%|████▍     | 11/25 [00:07<00:09,  1.49it/s]\u001b[A\n",
      "Evaluating:  48%|████▊     | 12/25 [00:08<00:08,  1.49it/s]\u001b[A\n",
      "Evaluating:  52%|█████▏    | 13/25 [00:08<00:08,  1.49it/s]\u001b[A\n",
      "Evaluating:  56%|█████▌    | 14/25 [00:09<00:07,  1.49it/s]\u001b[A\n",
      "Evaluating:  60%|██████    | 15/25 [00:10<00:06,  1.49it/s]\u001b[A\n",
      "Evaluating:  64%|██████▍   | 16/25 [00:10<00:06,  1.49it/s]\u001b[A\n",
      "Evaluating:  68%|██████▊   | 17/25 [00:11<00:05,  1.49it/s]\u001b[A\n",
      "Evaluating:  72%|███████▏  | 18/25 [00:12<00:04,  1.50it/s]\u001b[A\n",
      "Evaluating:  76%|███████▌  | 19/25 [00:12<00:04,  1.50it/s]\u001b[A\n",
      "Evaluating:  80%|████████  | 20/25 [00:13<00:03,  1.49it/s]\u001b[A\n",
      "Evaluating:  84%|████████▍ | 21/25 [00:14<00:02,  1.49it/s]\u001b[A\n",
      "Evaluating:  88%|████████▊ | 22/25 [00:14<00:02,  1.49it/s]\u001b[A\n",
      "Evaluating:  92%|█████████▏| 23/25 [00:15<00:01,  1.49it/s]\u001b[A\n",
      "Evaluating:  96%|█████████▌| 24/25 [00:16<00:00,  1.49it/s]\u001b[A\n",
      "Evaluating: 100%|██████████| 25/25 [00:16<00:00,  1.49it/s]\n",
      "\n",
      "Evaluating:   0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "Evaluating:   4%|▍         | 1/25 [00:00<00:16,  1.50it/s]\u001b[A\n",
      "Evaluating:   8%|▊         | 2/25 [00:01<00:15,  1.50it/s]\u001b[A\n",
      "Evaluating:  12%|█▏        | 3/25 [00:02<00:14,  1.49it/s]\u001b[A\n",
      "Evaluating:  16%|█▌        | 4/25 [00:02<00:14,  1.49it/s]\u001b[A\n",
      "Evaluating:  20%|██        | 5/25 [00:03<00:13,  1.49it/s]\u001b[A\n",
      "Evaluating:  24%|██▍       | 6/25 [00:04<00:12,  1.49it/s]\u001b[A\n",
      "Evaluating:  28%|██▊       | 7/25 [00:04<00:12,  1.49it/s]\u001b[A\n",
      "Evaluating:  32%|███▏      | 8/25 [00:05<00:11,  1.49it/s]\u001b[A\n",
      "Evaluating:  36%|███▌      | 9/25 [00:06<00:10,  1.50it/s]\u001b[A\n",
      "Evaluating:  40%|████      | 10/25 [00:06<00:10,  1.49it/s]\u001b[A\n",
      "Evaluating:  44%|████▍     | 11/25 [00:07<00:09,  1.49it/s]\u001b[A\n",
      "Evaluating:  48%|████▊     | 12/25 [00:08<00:08,  1.49it/s]\u001b[A\n",
      "Evaluating:  52%|█████▏    | 13/25 [00:08<00:08,  1.49it/s]\u001b[A\n",
      "Evaluating:  56%|█████▌    | 14/25 [00:09<00:07,  1.49it/s]\u001b[A\n",
      "Evaluating:  60%|██████    | 15/25 [00:10<00:06,  1.49it/s]\u001b[A\n",
      "Evaluating:  64%|██████▍   | 16/25 [00:10<00:06,  1.49it/s]\u001b[A\n",
      "Evaluating:  68%|██████▊   | 17/25 [00:11<00:05,  1.49it/s]\u001b[A\n",
      "Evaluating:  72%|███████▏  | 18/25 [00:12<00:04,  1.49it/s]\u001b[A\n",
      "Evaluating:  76%|███████▌  | 19/25 [00:12<00:04,  1.49it/s]\u001b[A\n",
      "Evaluating:  80%|████████  | 20/25 [00:13<00:03,  1.49it/s]\u001b[A\n",
      "Evaluating:  84%|████████▍ | 21/25 [00:14<00:02,  1.49it/s]\u001b[A\n",
      "Evaluating:  88%|████████▊ | 22/25 [00:14<00:02,  1.49it/s]\u001b[A\n",
      "Evaluating:  92%|█████████▏| 23/25 [00:15<00:01,  1.49it/s]\u001b[A\n",
      "Evaluating:  96%|█████████▌| 24/25 [00:16<00:00,  1.50it/s]\u001b[A\n",
      "Evaluating: 100%|██████████| 25/25 [00:16<00:00,  1.49it/s]\n",
      "epoch: 100%|██████████| 1/1 [02:16<00:00, 136.93s/it, Accuracy: Train 0.00%, Val 0.00%]\n",
      "/var/tmp/pbs.851483.pbs/ipykernel_3714866/2017306249.py:243: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"../trained_models/best_model.pth\", map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|██████████| 125/125 [01:23<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrices saved in: ../results/plots/confusion_matrices\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# create model and view the output \n",
    "\n",
    "batch = next(iter(train_dataloader))\n",
    "patch_size = 4\n",
    "\n",
    "train_images = batch['image']\n",
    "train_labels = batch['labels']\n",
    "\n",
    "# set channels_in to the number of channels of the dataset images (in our case we have 3 channels)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = ViT(image_size = train_images.shape[2], \n",
    "            channels_in = train_images.shape[1], \n",
    "            patch_size = patch_size, \n",
    "            hidden_size = 128, \n",
    "            num_layers = 8, \n",
    "            num_heads = 8).to(device)\n",
    "\n",
    "# pass an image through the network to check it works\n",
    "\n",
    "try:\n",
    "    out = model(train_images.to(device))\n",
    "    print(\"The model works with your data\")\n",
    "    print(\"Output shape:\", out.shape)  # Optional: Print the output shape\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)\n",
    "\n",
    "\n",
    "\n",
    "# set up the optimizer \n",
    "\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_epochs = 1\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, \n",
    "                                                    T_max = num_epochs, \n",
    "                                                    eta_min = 0)\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss() # note that we use this as we have multiple labels \n",
    "\n",
    "# define the training process ---------------------------\n",
    "\n",
    "def train(model, optimizer, loader, device, loss_fn, loss_logger):\n",
    "\n",
    "    # set network in train mode\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    for i, batch in enumerate(tqdm(loader, leave = True, desc = \"training\")):\n",
    "        # forward pass of image through network and get output \n",
    "\n",
    "        x = batch['image']\n",
    "        y = batch['labels']\n",
    "\n",
    "        fx = model(x.to(device))\n",
    "\n",
    "        # calculate loss using loss function \n",
    "\n",
    "        loss = loss_fn(fx, y.float().to(device)) # this requires correct float \n",
    "\n",
    "        # zero gradients \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # back propagate\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # single optimisation step \n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "    avg_loss = total_loss / num_batches if num_batches > 0 else 0  # Compute epoch loss\n",
    "    loss_logger.append(avg_loss)  # Store epoch loss\n",
    "\n",
    "    return model, optimizer, loss_logger\n",
    "\n",
    "\n",
    "# define the testing process\n",
    "\n",
    "def exact_match_accuracy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the exact match accuracy.\n",
    "    A sample is counted as correct only if all labels match exactly.\n",
    "    \"\"\"\n",
    "    return (y_true == y_pred).all(dim=1).float().mean().item()\n",
    "\n",
    "# This function should perform a single evaluation epoch, it WILL NOT be used to train our model\n",
    "def evaluate(model, device, loader):\n",
    "    \n",
    "    # Initialise counter\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    # Set network in evaluation mode\n",
    "    # Layers like Dropout will be disabled\n",
    "    # Layers like Batchnorm will stop calculating running mean and standard deviation\n",
    "    # and use current stored values (More on these layer types soon!)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        epoch_predicted_labels = []\n",
    "        epoch_ground_truth_labels = []\n",
    "\n",
    "        for i, batch in enumerate(tqdm(loader, leave=True, desc=\"Evaluating\")):\n",
    "\n",
    "                x = batch['image']\n",
    "                y = batch['labels']\n",
    "                # Forward pass of image through network\n",
    "                fx = model(x.to(device))\n",
    "                \n",
    "                preds = (torch.sigmoid(fx) > 0.5).float()\n",
    "                # Log the cumulative sum of the acc\n",
    "\n",
    "                epoch_predicted_labels.append(preds.cpu().numpy())\n",
    "                epoch_ground_truth_labels.append(y.cpu().numpy())\n",
    "\n",
    "        # Concatenate all batches\n",
    "        y_true_np = np.vstack(epoch_ground_truth_labels)\n",
    "        y_pred_np = np.vstack(epoch_predicted_labels)\n",
    "\n",
    "        exact_acc = accuracy_score(y_true_np, y_pred_np)\n",
    "                                             \n",
    "                    \n",
    "\n",
    "\n",
    "            \n",
    "    # Return the accuracy from the epoch     \n",
    "    return exact_acc\n",
    "\n",
    "\n",
    "\n",
    "# now the training process \n",
    "\n",
    "training_loss_logger = []\n",
    "validation_acc_logger = []\n",
    "training_acc_logger = []\n",
    "best_val_accuracy = 0\n",
    "best_model_path = \"../trained_models/best_model.pth\"  # Path to save the best model\n",
    "\n",
    "\n",
    "\n",
    "# this implements training loop \n",
    "\n",
    "pbar = trange(0, num_epochs, leave= True, desc = \"epoch\")\n",
    "\n",
    "for epoch in pbar: \n",
    "    valid_acc = 0\n",
    "    train_acc = 0\n",
    "\n",
    "    model, optimizer, training_loss_logger = train(model = model, \n",
    "                                                   optimizer = optimizer, \n",
    "                                                   loader = train_dataloader,\n",
    "                                                   device = device, \n",
    "                                                   loss_fn = loss_fn, \n",
    "                                                   loss_logger = training_loss_logger\n",
    "                                                   )\n",
    "    \n",
    "    # call evaluate function and pass dataloader for both validaiton and training \n",
    "\n",
    "    train_acc = evaluate(model = model, device = device, loader = train_dataloader)\n",
    "    valid_acc = evaluate(model = model, device = device, loader = val_dataloader) # note we are using exact match accuracy \n",
    "\n",
    "    \n",
    "\n",
    "    # log the train and validation accuracies \n",
    "\n",
    "    validation_acc_logger.append(valid_acc)\n",
    "    training_acc_logger.append(train_acc)\n",
    "\n",
    "    if valid_acc > best_val_accuracy:\n",
    "        best_val_accuracy = valid_acc\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"New best model saved with validation accuracy: {valid_acc:.4f}\")\n",
    "\n",
    "    # reduce the learning rate \n",
    "\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    pbar.set_postfix_str(\"Accuracy: Train %.2f%%, Val %.2f%%\" % (train_acc * 100, valid_acc * 100))\n",
    "\n",
    "print(\"Training complete\")\n",
    "\n",
    "dict = {\"training_loss\": training_loss_logger, \n",
    "        \"validation_accuracy\": validation_acc_logger, \n",
    "        \"training_accuracy\": training_acc_logger}\n",
    "\n",
    "training_logs = pd.DataFrame(dict)\n",
    "training_logs.to_csv(\"../results/training_logs.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Get class label mapping from Hugging Face dataset\n",
    "label_list = ds_test.features['labels'].feature.names  # List of string labels\n",
    "num_classes = len(label_list)\n",
    "\n",
    "# Initialize lists to store all predictions and labels\n",
    "all_true_labels = []\n",
    "all_pred_labels = []\n",
    "\n",
    "\n",
    "model = ViT(image_size = train_images.shape[2], \n",
    "            channels_in = train_images.shape[1], \n",
    "            patch_size = patch_size, \n",
    "            hidden_size = 128, \n",
    "            num_layers = 8, \n",
    "            num_heads = 8).to(device)\n",
    "\n",
    "# Load the saved state dictionary\n",
    "model.load_state_dict(torch.load(\"../trained_models/best_model.pth\", map_location=device))\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Loop over all batches in the dataloader\n",
    "for batch in tqdm(test_dataloader, leave= True, desc=\"Processing Batches\"):\n",
    "    \n",
    "    batch_image = batch['image']\n",
    "    batch_labels = batch['labels']  # True labels\n",
    "\n",
    "    with torch.no_grad():\n",
    "        fx = model(batch_image.to(device))  # Forward pass\n",
    "        pred = (torch.sigmoid(fx) > 0.5).float()  # Convert logits to binary labels\n",
    "\n",
    "    # Store batch labels and predictions\n",
    "    all_true_labels.append(batch_labels.cpu().numpy())  # Convert to NumPy and store\n",
    "    all_pred_labels.append(pred.cpu().int().numpy())\n",
    "\n",
    "# Convert lists to full NumPy arrays\n",
    "all_true_labels = np.vstack(all_true_labels)  # Shape: (num_samples, num_classes)\n",
    "all_pred_labels = np.vstack(all_pred_labels)  # Shape: (num_samples, num_classes)\n",
    "\n",
    "\n",
    "# Compute multi-label confusion matrix\n",
    "multi_cm = multilabel_confusion_matrix(all_true_labels, all_pred_labels)\n",
    "\n",
    "# Function to plot confusion matrices with labels\n",
    "def plot_and_save_confusion_matrix(cm, class_name, save_path):\n",
    "    \"\"\"\n",
    "    Plots and saves a single confusion matrix with labels.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.title(f\"Confusion Matrix for {class_name}\")\n",
    "\n",
    "    # Save the figure\n",
    "    file_name = f\"{save_path}/confusion_matrix_{class_name}.png\"\n",
    "    plt.savefig(file_name, bbox_inches='tight', dpi=300)\n",
    "    plt.close()  # Close the figure to free memory\n",
    "\n",
    "# Define the directory where to save the files\n",
    "save_directory = \"../results/plots/confusion_matrices\"\n",
    "\n",
    "# Ensure the directory exists\n",
    "\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "# Plot and save each confusion matrix with its class name\n",
    "for label, cm in zip(label_list, multi_cm):\n",
    "    plot_and_save_confusion_matrix(cm, label, save_directory)\n",
    "\n",
    "print(f\"Confusion matrices saved in: {save_directory}\")\n",
    "\n",
    "\n",
    "\n",
    "# Compute multi-label confusion matrix\n",
    "multi_cm = multilabel_confusion_matrix(all_true_labels, all_pred_labels)\n",
    "\n",
    "# Initialize lists to store per-class metrics\n",
    "class_names = label_list  # Class labels from dataset\n",
    "precision_list, recall_list, f1_list, accuracy_list = [], [], [], []\n",
    "\n",
    "# Compute per-class precision, recall, f1-score, accuracy\n",
    "for i, label in enumerate(class_names):\n",
    "    tn, fp, fn, tp = multi_cm[i].ravel()  # Extract TN, FP, FN, TP\n",
    "\n",
    "    # Compute Metrics\n",
    "    precision = tp / (tp + fp + 1e-8)  # Avoid division by zero\n",
    "    recall = tp / (tp + fn + 1e-8)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn + 1e-8)\n",
    "\n",
    "    # Append to lists\n",
    "    precision_list.append(precision)\n",
    "    recall_list.append(recall)\n",
    "    f1_list.append(f1)\n",
    "    accuracy_list.append(accuracy)\n",
    "\n",
    "# **Compute Exact Match Accuracy**\n",
    "exact_match = accuracy_score(all_true_labels, all_pred_labels)  # Computes exact match accuracy\n",
    "\n",
    "# Create DataFrame\n",
    "df_metrics = pd.DataFrame({\n",
    "    \"Class\": class_names,\n",
    "    \"Precision\": precision_list,\n",
    "    \"Recall\": recall_list,\n",
    "    \"F1-Score\": f1_list,\n",
    "    \"Accuracy\": accuracy_list\n",
    "})\n",
    "\n",
    "# Add a row for Exact Match Accuracy (overall model accuracy)\n",
    "df_metrics.loc[len(df_metrics)] = [\"Exact Match Accuracy\", \"\", \"\", \"\", exact_match]\n",
    "\n",
    "df_metrics.to_csv(\"../results/df/df_metrics_overall.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
