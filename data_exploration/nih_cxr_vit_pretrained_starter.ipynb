{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rds/general/user/ojf24/home/anaconda3/envs/pytorch_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "import torch \n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "from tqdm import trange, tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "import os \n",
    "from PIL import Image\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uninstalling torch-1.1.0:\n",
      "\u001b[31mERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"/apps/jupyterhub/2019-04-29/miniconda/lib/python3.6/shutil.py\", line 550, in move\n",
      "    os.rename(src, real_dst)\n",
      "OSError: [Errno 18] Invalid cross-device link: '/rds/general/applications/jupyterhub/2019-04-29/miniconda/lib/python3.6/site-packages/caffe2' -> '/var/tmp/pbs.863968.pbs/pip-uninstall-rz6zdgtu'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/apps/jupyterhub/2019-04-29/miniconda/lib/python3.6/site-packages/pip/_internal/cli/base_command.py\", line 178, in main\n",
      "    status = self.run(options, args)\n",
      "  File \"/apps/jupyterhub/2019-04-29/miniconda/lib/python3.6/site-packages/pip/_internal/commands/uninstall.py\", line 75, in run\n",
      "    auto_confirm=options.yes, verbose=self.verbosity > 0,\n",
      "  File \"/apps/jupyterhub/2019-04-29/miniconda/lib/python3.6/site-packages/pip/_internal/req/req_install.py\", line 825, in uninstall\n",
      "    uninstalled_pathset.remove(auto_confirm, verbose)\n",
      "  File \"/apps/jupyterhub/2019-04-29/miniconda/lib/python3.6/site-packages/pip/_internal/req/req_uninstall.py\", line 388, in remove\n",
      "    moved.stash(path)\n",
      "  File \"/apps/jupyterhub/2019-04-29/miniconda/lib/python3.6/site-packages/pip/_internal/req/req_uninstall.py\", line 277, in stash\n",
      "    renames(path, new_path)\n",
      "  File \"/apps/jupyterhub/2019-04-29/miniconda/lib/python3.6/site-packages/pip/_internal/utils/misc.py\", line 305, in renames\n",
      "    shutil.move(old, new)\n",
      "  File \"/apps/jupyterhub/2019-04-29/miniconda/lib/python3.6/shutil.py\", line 562, in move\n",
      "    rmtree(src)\n",
      "  File \"/apps/jupyterhub/2019-04-29/miniconda/lib/python3.6/shutil.py\", line 486, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/apps/jupyterhub/2019-04-29/miniconda/lib/python3.6/shutil.py\", line 424, in _rmtree_safe_fd\n",
      "    _rmtree_safe_fd(dirfd, fullname, onerror)\n",
      "  File \"/apps/jupyterhub/2019-04-29/miniconda/lib/python3.6/shutil.py\", line 444, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/apps/jupyterhub/2019-04-29/miniconda/lib/python3.6/shutil.py\", line 442, in _rmtree_safe_fd\n",
      "    os.unlink(name, dir_fd=topfd)\n",
      "PermissionError: [Errno 13] Permission denied: '__init__.py'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = load_dataset(\"alkzar90/NIH-Chest-X-ray-dataset\", 'image-classification', split = \"train[:5000]\") \n",
    "\n",
    "# we hold back our test data to be used purely for testing, not in the context of our training loop \n",
    "ds_test = load_dataset(\"alkzar90/NIH-Chest-X-ray-dataset\", 'image-classification', split = \"test[:2000]\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "from torch import optim \n",
    "\n",
    "from transformers import ViTForImageClassification\n",
    "import torchmetrics \n",
    "\n",
    "import lightning as L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continue using pytorch ligthing to fine tune model as per \n",
    "\n",
    "# https://towardsdatascience.com/how-to-fine-tune-a-pretrained-vision-transformer-on-satellite-data-d0ddd8359596/#:~:text=Under%20the%20hood%2C%20the%20trainer,is%20completed%20within%20few%20epochs.\n",
    "\n",
    "\n",
    "class VisionTransformerPretrained(L.LightningModule): \n",
    "    '''wrapper for the pretrained vision transformers'''\n",
    "\n",
    "    def __init__(self, model = \"google/vit-base-patch16-224\", num_classes = 15, learning_rate = 1e-4):\n",
    "\n",
    "        super().__init__()\n",
    "        self.learning_rate = learning_rate \n",
    "        self.num_classes = num_classes\n",
    "        backbone = ViTForImageClassification.from_pretrained(model, \n",
    "                                                             num_labels = num_classes, \n",
    "                                                             ignore_mismatched_sizes=True)\n",
    "        \n",
    "        self.backbone = backbone \n",
    "\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss() # adjusted for our task of multilabel \n",
    "\n",
    "        #metrics \n",
    "\n",
    "        self.acc = torchmetrics.Accuracy(\"multilabel\", num_labels=num_classes, threshold = 0.5)\n",
    "\n",
    "        self.f1 = torchmetrics.F1Score(task=\"multilabel\", num_labels=num_classes, average=None)  # Per-label F1\n",
    "        self.precision = torchmetrics.Precision(task=\"multilabel\", num_labels=num_classes, average=None)\n",
    "        self.recall = torchmetrics.Recall(task=\"multilabel\", num_labels=num_classes, average=None)\n",
    "        self.accuracy = torchmetrics.Accuracy(task=\"multilabel\", num_labels=num_classes, average=None)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x): \n",
    "        return self.backbone(x).logits\n",
    "    \n",
    "    def step(self, batch, stage = \"train\"):\n",
    "        '''Any step proccesses to return loss and predictions'''\n",
    "\n",
    "        x, y = batch \n",
    "\n",
    "        logits = self.forward(x)\n",
    "        y_hat = (torch.sigmoid(logits)>0.5).float() # we need to \n",
    "\n",
    "        loss = self.loss_fn(logits, y.float())\n",
    "        acc = self.acc(y_hat, y)\n",
    "\n",
    "        return loss, acc, y_hat, y\n",
    "    \n",
    "    def training_step(self, batch, batch_idx): \n",
    "        loss, acc, y_hat, y = self.step(batch)\n",
    "\n",
    "        self.log(\"train_loss\", loss)\n",
    "\n",
    "        return loss \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx): \n",
    "        loss, acc, y_hat, y = self.step(batch)\n",
    "\n",
    "        self.log(\"valid_acc\", acc, on_epoch = True, on_step = False)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr = 1e-4)\n",
    "        return optimizer \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuggingFaceCXR(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset wrapper for Hugging Face datasets.\n",
    "    Converts dataset samples into a PyTorch-compatible format.\n",
    "    \"\"\"\n",
    "    def __init__(self, hf_dataset, image_size, num_classes=15, transform=None):\n",
    "        self.dataset = hf_dataset.with_format(\"torch\")  # Ensure dataset is in torch format\n",
    "        self.image_size = image_size\n",
    "        self.num_classes = num_classes\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "\n",
    "        # Load image\n",
    "        image = item[\"image\"]  # Ensure this matches your dataset keys\n",
    "\n",
    "        # Resize image\n",
    "        image = F.interpolate(image.unsqueeze(0), size = self.image_size, mode = \"bilinear\").squeeze(0)\n",
    "\n",
    "        # Handle 4-channel (RGBA) images: Keep only RGB\n",
    "        if image.shape[0] == 4:\n",
    "            image = image[:3, :, :]\n",
    "\n",
    "        # Handle Grayscale (1-channel) images: Convert to 3-channel\n",
    "        if image.shape[0] == 1:\n",
    "            image = image.repeat(3, 1, 1)\n",
    "\n",
    "        # Normalize pixel values to [0,1]\n",
    "        image = image / 255.0\n",
    "\n",
    "        # Convert labels to one-hot encoding\n",
    "        labels = item[\"labels\"]\n",
    "        one_hot = torch.zeros(self.num_classes, dtype=torch.float32)\n",
    "        one_hot[labels] = 1  # Set corresponding indices to 1\n",
    "\n",
    "        # Apply optional transformations\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "import lightning as L\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "class nih_cxr_datamodule(L.LightningDataModule):\n",
    "    '''Lightning data module for the cxr dataset'''\n",
    "\n",
    "    def __init__(self, batch_size, data_root=\"alkzar90/NIH-Chest-X-ray-dataset\"): \n",
    "        super().__init__()\n",
    "        self.data_root = data_root\n",
    "        self.batch_size = batch_size \n",
    "        self.num_classes = 15\n",
    "\n",
    "    def setup(self, stage = None):\n",
    "        '''set up the dataset, train/valid/test all at once'''\n",
    "\n",
    "        transforms = v2.Compose([v2.ToImage(),\n",
    "                                 v2.Resize(size=(224,224), interpolation=2),\n",
    "                                 v2.Grayscale(num_output_channels=3), # need to ensure 3 channel grayscale for vit \n",
    "                                 v2.ToDtype(torch.float32, scale=True),\n",
    "                                 v2.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "                                ])\n",
    "        \n",
    "        ds_train = load_dataset(self.data_root, 'image-classification', split = \"train[:5000]\")\n",
    "\n",
    "        train_valid_split = ds_train.train_test_split(test_size = 0.2)\n",
    "\n",
    "        ds_train = train_valid_split['train']\n",
    "        ds_valid = train_valid_split['test']\n",
    "\n",
    " \n",
    "        ds_test = load_dataset(self.data_root, 'image-classification', split = \"test[:2000]\") \n",
    "\n",
    "        self.train_data = HuggingFaceCXR(ds_train, image_size = (224, 224), transform = transforms)\n",
    "        self.valid_data = HuggingFaceCXR(ds_valid, image_size = (224, 224), transform = transforms)\n",
    "        self.test_data = HuggingFaceCXR(ds_test, image_size = (224, 224), transform = transforms)\n",
    "\n",
    "\n",
    "    def train_dataloader(self): \n",
    "        return DataLoader(self.train_data, batch_size = self.batch_size, shuffle = True)\n",
    "    \n",
    "    def valid_dataloader(self): \n",
    "        return DataLoader(dataset = self.valid_data, batch_size = self.batch_size, shuffle = False)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_data, batch_size = self.batch_size, shuffle = False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "def multi_label_evaluation(model, test_dataloader, test_dataset, logger): \n",
    "    model.eval()\n",
    "\n",
    "    all_true_labels = []\n",
    "    all_pred_logits = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dataloader, desc = \"Collecting logits\"): \n",
    "            x, y = batch \n",
    "            logits = model(x)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "            all_true_labels.append(y.cpu().numpy())\n",
    "            all_pred_logits.append(probs) # store these so we can assess different thresholds quickly \n",
    "\n",
    "    all_true_labels = np.vstack(all_true_labels)\n",
    "    all_pred_logits = np.vstack(all_pred_logits)\n",
    "\n",
    "    label_list = test_dataset.features['labels'].feature.names\n",
    "\n",
    "    # now we try and find our best threshold before classificaiton report at that threshold \n",
    "    thresholds_to_test = np.linspace(0, 1, 10)\n",
    "    f1_micro_list = []\n",
    "\n",
    "    for value in thresholds_to_test:\n",
    "        predictions = (all_pred_logits >= value).astype(int)\n",
    "        report = classification_report(all_true_labels, predictions, target_names = label_list, output_dict = True)\n",
    "        micro_f1_average = report['micro avg']['f1-score']\n",
    "        f1_micro_list.append({\"Threshold\": value, \"Micro-F1\": micro_f1_average})\n",
    "\n",
    "    # save micro f1 scores \n",
    "    log_dir = logger.log_dir\n",
    "\n",
    "    os.makedirs(log_dir, exist_ok = True)\n",
    "    f1_micro_path = os.path.join(log_dir, \"f1_micro_average.csv\")\n",
    "    f1_micro_average_df = pd.DataFrame(f1_micro_list)\n",
    "    f1_micro_average_df.to_csv(f1_micro_path, index = False)\n",
    "   \n",
    "    # now select the threshold that gave the highest micro average \n",
    "\n",
    "    best_threshold = f1_micro_average_df.loc[f1_micro_average_df[\"Micro-F1\"].idxmax(), \"Threshold\"]\n",
    "\n",
    "    # calculate final labels based on best threshold \n",
    "\n",
    "    all_pred_labels = (all_pred_logits >= best_threshold).astype(int)\n",
    "    \n",
    "    report_path = os.path.join(log_dir, f\"test_multi_metrics_{best_threshold:.4f}.csv\")\n",
    "    final_report = classification_report(all_true_labels, all_pred_labels, target_names=label_list, zero_division=0, output_dict = True)\n",
    "    pd.DataFrame(final_report).to_csv(report_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def main(args): \n",
    "    L.seed_everything(42)\n",
    "\n",
    "    # set up data \n",
    "    datamodule = nih_cxr_datamodule(batch_size=8)\n",
    "    datamodule.prepare_data()\n",
    "    datamodule.setup()\n",
    "\n",
    "    train_dataloader = datamodule.train_dataloader()\n",
    "    valid_dataloader = datamodule.valid_dataloader()\n",
    "    test_dataloader = datamodule.test_dataloader()\n",
    "\n",
    "    # setup model \n",
    "    model = VisionTransformerPretrained('google/vit-base-patch16-224', datamodule.num_classes, learning_rate= 1e-4)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor = 'valid_acc', patience = 6, mode = 'max')\n",
    "\n",
    "    logger = CSVLogger(\"tensorboard_logs\", name = 'nih_cxr_pretrained_vit')\n",
    "\n",
    "    #train \n",
    "    trainer = L.Trainer(devices = 1, max_epochs = 10, callbacks = [early_stopping], logger =logger)\n",
    "    trainer.fit(model = model, train_dataloaders=train_dataloader, val_dataloaders = valid_dataloader)\n",
    "\n",
    "    # evaluate on the test set \n",
    "    \n",
    "    # we want to set our threshold based on micro average due to class imbalance - use micro average f1 score \n",
    "\n",
    "    multi_label_evaluation(model, test_dataloader = test_dataloader, \n",
    "                           test_dataset = ds_test, logger = logger)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([15]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([15, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m args = \u001b[33m\"\u001b[39m\u001b[33marg\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m run = \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m#train \u001b[39;00m\n\u001b[32m     26\u001b[39m trainer = L.Trainer(devices = \u001b[32m1\u001b[39m, max_epochs = \u001b[32m10\u001b[39m, callbacks = [early_stopping], logger =logger)\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# evaluate on the test set \u001b[39;00m\n\u001b[32m     30\u001b[39m \n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# we want to set our threshold based on micro average due to class imbalance - use micro average f1 score \u001b[39;00m\n\u001b[32m     33\u001b[39m multi_label_evaluation(model, test_dataloader = test_dataloader, \n\u001b[32m     34\u001b[39m                        test_dataset = ds_test, logger = logger)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pytorch_env/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:539\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28mself\u001b[39m.state.status = TrainerStatus.RUNNING\n\u001b[32m    538\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m539\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    540\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[32m    541\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pytorch_env/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:47\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trainer.strategy.launcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     46\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[32m     50\u001b[39m     _call_teardown_hook(trainer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pytorch_env/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:575\u001b[39m, in \u001b[36mTrainer._fit_impl\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    568\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    569\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    570\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn,\n\u001b[32m    571\u001b[39m     ckpt_path,\n\u001b[32m    572\u001b[39m     model_provided=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    573\u001b[39m     model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    574\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m575\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    577\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n\u001b[32m    578\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pytorch_env/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:958\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m    955\u001b[39m \u001b[38;5;28mself\u001b[39m._logger_connector.reset_metrics()\n\u001b[32m    957\u001b[39m \u001b[38;5;66;03m# strategy will configure model and move it to the device\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m958\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m.\u001b[49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[38;5;66;03m# hook\u001b[39;00m\n\u001b[32m    961\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.fn == TrainerFn.FITTING:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pytorch_env/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py:155\u001b[39m, in \u001b[36mStrategy.setup\u001b[39m\u001b[34m(self, trainer)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;66;03m# let the precision plugin convert the module here so that this strategy hook can decide the order\u001b[39;00m\n\u001b[32m    153\u001b[39m \u001b[38;5;66;03m# of operations\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;28mself\u001b[39m.model = \u001b[38;5;28mself\u001b[39m.precision_plugin.convert_module(\u001b[38;5;28mself\u001b[39m.model)\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[38;5;28mself\u001b[39m.model = \u001b[38;5;28mself\u001b[39m._setup_model(\u001b[38;5;28mself\u001b[39m.model)\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer.state.fn == TrainerFn.FITTING:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pytorch_env/lib/python3.11/site-packages/lightning/pytorch/strategies/single_device.py:79\u001b[39m, in \u001b[36mSingleDeviceStrategy.model_to_device\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmodel_to_device\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     78\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mself.model must be set before self.model.to()\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mroot_device\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pytorch_env/lib/python3.11/site-packages/lightning/fabric/utilities/device_dtype_mixin.py:55\u001b[39m, in \u001b[36m_DeviceDtypeModuleMixin.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     53\u001b[39m device, dtype = torch._C._nn._parse_to(*args, **kwargs)[:\u001b[32m2\u001b[39m]\n\u001b[32m     54\u001b[39m _update_properties(\u001b[38;5;28mself\u001b[39m, device=device, dtype=dtype)\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pytorch_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1343\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1340\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1341\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pytorch_env/lib/python3.11/site-packages/torch/nn/modules/module.py:903\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    902\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    907\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    908\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    914\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pytorch_env/lib/python3.11/site-packages/torch/nn/modules/module.py:903\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    902\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    907\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    908\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    914\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[31m[... skipping similar frames: Module._apply at line 903 (2 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pytorch_env/lib/python3.11/site-packages/torch/nn/modules/module.py:903\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    902\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    907\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    908\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    914\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pytorch_env/lib/python3.11/site-packages/torch/nn/modules/module.py:930\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    926\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    927\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    928\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    929\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    931\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    933\u001b[39m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pytorch_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1329\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1322\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1323\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1324\u001b[39m             device,\n\u001b[32m   1325\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1326\u001b[39m             non_blocking,\n\u001b[32m   1327\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1328\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1329\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1333\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1334\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1335\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "args = \"arg\"\n",
    "run = main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['epoch', 'step', 'train_loss', 'val_multi_label_f1'], dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "training_metrics = pd.read_csv(\"/Users/oliverfox/git_repositories/nih_cxr_project/vit_pretrained_fine_tune/tensorboard_logs/nih_cxr_pretrained_vit/version_1/metrics.csv\")\n",
    "training_metrics.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_multi_label_f1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.342471</td>\n",
       "      <td>0.196633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.233276</td>\n",
       "      <td>0.213044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.127981</td>\n",
       "      <td>0.246632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.244295</td>\n",
       "      <td>0.303175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.098595</td>\n",
       "      <td>0.380172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       train_loss  val_multi_label_f1\n",
       "epoch                                \n",
       "0        0.342471            0.196633\n",
       "1        0.233276            0.213044\n",
       "2        0.127981            0.246632\n",
       "3        0.244295            0.303175\n",
       "4        0.098595            0.380172"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_metrics = training_metrics.groupby(training_metrics['epoch']).aggregate({'train_loss':'first', 'val_multi_label_f1':\"first\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: ylabel='train_loss'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns \n",
    "\n",
    "sns.lineplot(training_metrics['train_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
